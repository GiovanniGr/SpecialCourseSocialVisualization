{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b816968d",
   "metadata": {},
   "source": [
    "# Report for Reddit ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68b948",
   "metadata": {},
   "source": [
    "In this report, I will go thruogh each step I followed in this project. In particular, the project has been ideally organized in 5 sections: \n",
    "  1. Idealization\n",
    "  2. Data Retrieving and Networks creation\n",
    "  3. Statistics analysis\n",
    "  4. Machine Learning classification\n",
    "  5. Web visualization\n",
    "  \n",
    "As mentioned before, the different sections are only logically ordered, but they have to be considered circular steps in which for each, updates and changes have been made to all the other sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a75b5e",
   "metadata": {},
   "source": [
    "###    1. Idealization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cd42d",
   "metadata": {},
   "source": [
    "The idea of this project is to study a number of different characteristics for many-to-many asynchronous networks structures and to try to classify a given network, based on those statistics, into different categories.\n",
    "\n",
    "The networks will be the discussions in [Reddit](https://www.reddit.com). Each discussion will create a comment network (i.e. the tree created by the comments in the discussion) and an author network (i.e. the graph created by the authors of the comments, linked if the replied to each other).\n",
    "\n",
    "It has been chosen to focus on two categories (i.e. two subreddits). As a starting point, the two subreddits will be very different two enhance the inter-separation, and in this case will be [\"puppies\"](https://www.reddit.com/r/puppies/) and [\"PoliticalDiscussion\"](https://www.reddit.com/r/PoliticalDiscussion/).\n",
    "\n",
    "The statistics analyzed are focused on three macro categories: the graph features derived by the author graph, the tree features from the comment trees and some time features from the life span of the discussion. Each feature will be analized more in detail.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce8e9a",
   "metadata": {},
   "source": [
    "### 2. Data Retrieving and Networks creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375dc10",
   "metadata": {},
   "source": [
    "We chose to use Reddit because it has the best APIs for retrieving data, and that it is actually possible to fetch all the comments and the discussions with less restrictions with respect to other social networks. For this reason, a huge amount of data is available.\n",
    "\n",
    "In this case, I tried two different approaches: fetch the data needed with the [APIs](https://www.reddit.com/dev/api/) by Reddit and retrieve the data needed from [historical databases](https://files.pushshift.io/reddit/).\n",
    "\n",
    "In this notebook, I show how to use the first method. This method have been reached after numerous attempts.\n",
    "\n",
    "In this notebook, a second method is used to retrieved the data. In particular, the data is fetched from databases updated every month of all the submissions done on subreddit. From those databases is possible then to retrieve the discussions and filter the wanted.\n",
    "\n",
    "The networks are then created by linking the nodes (i.e. comments or authors) by the replies of each comment in the discussion. All these processes are described in the notebooks above. \n",
    "\n",
    "This section took a lot of time to be completed. The first method allows for a better control of the data fetched and a finer customisation. On the other hand, limits on the number and frequency of request actios make this system not scalable for big amount of data. With the second method, all the submissions of a month are downloaded and it is possible then to filter only the subreddits of interest. In this way there is a big waste of resources but the entire process is faster if there is the necessity of having data for several months and a big amount of submissions.\n",
    "\n",
    "This section gave me the possibility to study and work with the APIs in general and for Reddit in particular. In addition, it allowed me to work with json files and the networkx library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97d6f5",
   "metadata": {},
   "source": [
    "### 3. Statistics analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "306570c7",
   "metadata": {},
   "source": [
    "In this section we enter in the hot part of the project. While creating the networks, a number of features have been computed and stored. In particular, for each submission, three categories of features have been collected:\n",
    "\n",
    "1. Author specific stats:\n",
    "  * number users\n",
    "  * number edges\n",
    "  * dimension largest connected component\n",
    "  * reciprocity value\n",
    "  * diameter\n",
    "  * cluster coefficient\n",
    "    \n",
    "    \n",
    "2. Comments specific stats:\n",
    "  * number comments\n",
    "  * number direct comments\n",
    "  * max depth\n",
    "  * H-index\n",
    "  \n",
    "\n",
    "3. Lifetime related stats:\n",
    "  * lifetime\n",
    "  * average time\n",
    "  * median time\n",
    "  * lifetime 95 percentile\n",
    "  \n",
    "This notebook plots the histograms for each feature.\n",
    "\n",
    "Many different steps contributed to the final result. In particular, many efforts have been focused on the choice of the submissions to filter out. If initially the idea was to take all the submissions, independently on the number of comments, we progressively gave more importance to this factor and eventually we greatly relied on it.\n",
    "\n",
    "In particular, the procedure followed is the one described [here!!!!!] (where \"cascade\" is our submissions, \"false-news\" and \"true-news\" are the two categories \"puppies\" and \"PoliticalDiscussion\" and \"size\" is the number of comments) *To create a size-matched corpus of cascades, we consider each true-news cascade and randomly sample a matching cascade of false news, uniformly at random with replacement, from the set of false-news cascades of the same size. If no false- news cascade of the same size exists (an occurrence affecting only a relatively small fraction of cascades in both datasets, but not necessarily in a more general setting), the true cascade is not included in the subsampled cascade corpus. We refer to this process as matching false cascades to true cascades. The result is two subsampled cascade corpuses with the exact same distribution of sizes; the included cascade sizes are a subset of the cascade sizes in the original datasets.*\"\n",
    "\n",
    "A detailed representation of all the features can be found in this notebook. Below, the most interesting features are shown.\n",
    "\n",
    "![28.png](https://images.emojiterra.com/google/android-10/512px/1f3e0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117df574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
