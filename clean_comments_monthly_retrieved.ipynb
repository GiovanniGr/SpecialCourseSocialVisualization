{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data retrieved from the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from fa2 import ForceAtlas2  #the package should be installed before\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from statistics import mean, median, quantiles\n",
    "from networkx.algorithms.traversal.breadth_first_search import descendants_at_distance\n",
    "import time\n",
    "import requests\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import zstandard\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save(file_paths, subreddit, subDescr= \"\", comments=True):\n",
    "    out_df = pd.DataFrame()\n",
    "    for count, file in enumerate(file_paths):\n",
    "        print(\"working on file nr. \",count+1,\"/\",len(file_paths))\n",
    "        df = pd.read_csv(file)\n",
    "        if comments==True:\n",
    "            df_cleaned = df[[\"score\", \"author_fullname\",\"body\",\"created_utc\",\"id\",\"parent_id\",\"subreddit\", \"subreddit_id\"]]\n",
    "            df_cleaned_sub = df_cleaned[ (df_cleaned[\"subreddit\"]==subreddit) & (df_cleaned[\"body\"]!= \"[removed]\") & (df_cleaned[\"body\"]!= \"[deleted]\")]\n",
    "            df_cleaned_sub[\"id\"]=\"t1_\"+df_cleaned_sub[\"id\"]\n",
    "            df_cleaned_sub.dropna(inplace=True)\n",
    "            df_cleaned_sub[[\"score\",\"created_utc\"]] = df_cleaned_sub[[\"score\",\"created_utc\"]].apply(pd.to_numeric)\n",
    "        else:\n",
    "            df_cleaned = df[[\"score\", \"author_fullname\",\"title\",\"num_comments\",\"created_utc\",\"id\",\"subreddit\", \"subreddit_id\"]]\n",
    "            df_cleaned_sub = df_cleaned[ (df_cleaned[\"subreddit\"]==subreddit) ]\n",
    "            df_cleaned_sub.dropna(inplace=True)\n",
    "            df_cleaned_sub[\"name\"] = \"t3_\"+df_cleaned_sub[\"id\"]\n",
    "            df_cleaned_sub[[\"score\",\"created_utc\", \"num_comments\"]] = df_cleaned_sub[[\"score\",\"created_utc\", \"num_comments\"]].apply(pd.to_numeric)\n",
    "        print(df_cleaned_sub.shape[0])\n",
    "        out_df= pd.concat([out_df,df_cleaned_sub], axis=0)\n",
    "    out_df.reset_index(drop=True, inplace=True)\n",
    "    if comments==True:\n",
    "        out_df.to_pickle(\"RC_\"+subreddit+subDescr+\".pickle\")\n",
    "    else:\n",
    "        out_df.to_pickle(\"RS_\"+subreddit+subDescr+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file nr.  1 / 36\n",
      "22\n",
      "working on file nr.  2 / 36\n",
      "29\n",
      "working on file nr.  3 / 36\n",
      "50\n",
      "working on file nr.  4 / 36\n",
      "39\n",
      "working on file nr.  5 / 36\n",
      "21\n",
      "working on file nr.  6 / 36\n",
      "93\n",
      "working on file nr.  7 / 36\n",
      "56\n",
      "working on file nr.  8 / 36\n",
      "55\n",
      "working on file nr.  9 / 36\n",
      "28\n",
      "working on file nr.  10 / 36\n",
      "40\n",
      "working on file nr.  11 / 36\n",
      "73\n",
      "working on file nr.  12 / 36\n",
      "46\n",
      "working on file nr.  13 / 36\n",
      "56\n",
      "working on file nr.  14 / 36\n",
      "47\n",
      "working on file nr.  15 / 36\n",
      "48\n",
      "working on file nr.  16 / 36\n",
      "55\n",
      "working on file nr.  17 / 36\n",
      "29\n",
      "working on file nr.  18 / 36\n",
      "29\n",
      "working on file nr.  19 / 36\n",
      "63\n",
      "working on file nr.  20 / 36\n",
      "40\n",
      "working on file nr.  21 / 36\n",
      "80\n",
      "working on file nr.  22 / 36\n",
      "48\n",
      "working on file nr.  23 / 36\n",
      "55\n",
      "working on file nr.  24 / 36\n",
      "56\n",
      "working on file nr.  25 / 36\n",
      "25\n",
      "working on file nr.  26 / 36\n",
      "57\n",
      "working on file nr.  27 / 36\n",
      "52\n",
      "working on file nr.  28 / 36\n",
      "58\n",
      "working on file nr.  29 / 36\n",
      "58\n",
      "working on file nr.  30 / 36\n",
      "45\n",
      "working on file nr.  31 / 36\n",
      "30\n",
      "working on file nr.  32 / 36\n",
      "60\n",
      "working on file nr.  33 / 36\n",
      "33\n",
      "working on file nr.  34 / 36\n",
      "76\n",
      "working on file nr.  35 / 36\n",
      "28\n",
      "working on file nr.  36 / 36\n",
      "38\n",
      "working on file nr.  1 / 6\n",
      "481\n",
      "working on file nr.  2 / 6\n",
      "420\n",
      "working on file nr.  3 / 6\n",
      "403\n",
      "working on file nr.  4 / 6\n",
      "363\n",
      "working on file nr.  5 / 6\n",
      "327\n",
      "working on file nr.  6 / 6\n",
      "343\n"
     ]
    }
   ],
   "source": [
    "subfolder = \"2019-2\"\n",
    "subred = \"puppies\"\n",
    "file_paths = glob.glob(\"monthly_archive/dataRetrieved/\"+subfolder+\"/RC_*\")\n",
    "clean_and_save(file_paths,subreddit=subred,subDescr=\"_\"+subfolder,comments=True)\n",
    "file_paths = glob.glob(\"monthly_archive/dataRetrieved/\"+subfolder+\"/RS_*\")\n",
    "clean_and_save(file_paths,subreddit=subred,subDescr=\"_\"+subfolder,comments=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
